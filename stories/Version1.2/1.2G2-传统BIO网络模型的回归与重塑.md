# **研发日志 1.2G2：反璞归真——传统 BIO 网络模型的回归与重塑**

大家好，  
在 1.2F24 版本中，我们成功推出了复杂的“进程切换（Process Switching）”模型，解决了玩家从大厅进入游戏世界的资源加载问题。然而，随着功能的实装，我们不得不面对一个尴尬的现实：我们的网络层代码变得越来越难以维护了。  
当我们试图用传统的回调（Callback）或纯异步（Netty-style）思维去处理包含 6\~7 个阶段的登录握手流程时，代码结构迅速崩塌成了“回调地狱”。  
在 1.2G2 中，我们做出了一个违背现代高性能网络编程常识的决定：**我们放弃了 Netty 那样的纯异步模型，转而全面回归同步阻塞的 BIO (Blocking I/O) 模型。**  
得益于 Java 21 虚拟线程（Virtual Threads）的加持，这次回归不是倒退，而是一次降维打击。

## **一、 为什么选择 BIO？**

在传统的物理线程时代，BIO 是性能杀手。一个连接占用一个线程，几千个线程就能把服务器拖垮。因此，NIO (Non-blocking I/O) 和 Netty 成为了标准答案。  
但是，NIO 带来了巨大的**心智负担**。你必须将一个线性的业务逻辑（比如：认证 \-\> 下载配置 \-\> 加载世界 \-\> 进入游戏）切碎成无数个小的 Handler 或 Callback。  
但在 Virtual Threads 时代，阻塞不再昂贵。  
我们可以为每一个玩家连接（NetworkSession）分配一个极其廉价的虚拟线程。在这个线程里，我们可以放心地调用 read() 或 Future.get() 进行阻塞，操作系统会自动调度，不会浪费 CPU。  
这意味着：**我们可以像写单机代码一样，从上到下线性地编写网络逻辑。**

## **二、 RPC 调用：让网络像本地方法一样**

为了配合这种线性思维，我们重构了底层的 RpcSession。  
以前，发送请求和接收响应是在不同的地方处理的。现在，我们引入了 RpcRequest 和 RpcResponse 封装。  
你可以像调用本地函数一样调用远程逻辑：  
// 旧有的异步/回调思维 (伪代码)  
send(new AuthPacket(), (response) \-\> {  
    if (response.success) {  
        send(new LoadPacket());  
    }  
});

// 1.2G2 的 RPC 思维  
var authResult \= session.rpcRequest(new AuthNDto(name, version)).get(); // 阻塞等待结果  
if (authResult.isSuccess()) {  
    // 继续下一步...  
}

底层通过 CompletableFuture 和 ConcurrentHashMap 自动匹配 RequestID，将异步的网络包重新“缝合”为同步的函数调用。

## **三、 革命性的 waitFor：编排异步的终极武器**

RPC 解决了“请求-响应”模型，但游戏网络中还有大量非 RPC 的单向通知（例如：服务器通知客户端“开始加载”，等待客户端回应“加载完成”）。  
为此，我们在 RpcSession 中实装了革命性的 **waitFor** 原语。  
它允许开发者在当前线程**阻塞等待**特定类型的下一个数据包。如果收到的包不匹配，它会暂存起来；直到收到期望的包，或者超时。  
让我们看看 1.2G2 中真实的玩家登录代码（摘自 ClientPsHandler）：  
// 1\. 发送批数据  
session.sendNext(BatchDataNVo.of(-1, "BatchData".getBytes()));

// 2\. 阻塞等待客户端确认 (就像在写本地逻辑一样自然！)  
// 在这一行代码返回之前，虚拟线程会挂起，直到客户端发来 BatchDataFinishNDto  
session.waitFor(BatchDataFinishNDto.class, 5, TimeUnit.MINUTES);

// 3\. 通知进程切换  
session.sendNext(PsNVo.of(...));

// 4\. 再次阻塞等待客户端允许  
session.waitFor(PsAllowNDto.class, 5, TimeUnit.MINUTES);

// 5\. 发送世界数据...

这种代码的可读性是惊人的。我们不需要在各种 PacketHandler 之间跳转，整个登录流程在一个方法内清晰地展现出来。

## **四、 Session 线程模型：三位一体的虚拟线程架构**

为了支撑上述的阻塞式逻辑，我们为每个 NetworkSession 设计了一套\*\*“三线程”并发模型\*\*。  
或许有人会问：“一个玩家占用 3 个线程？这也太奢侈了吧！”  
在物理线程时代，这确实是疯了。但对于虚拟线程（Virtual Thread）来说，创建 30,000 个线程（支持 10,000 在线玩家）的内存开销仅相当于几十兆字节，调度开销几乎为零。  
因此，我们设计了如下架构：

1. **VT1 \- 接收线程 (IO-Receive / The Ear)**  
   * **职责**：运行 RpcSession.receiveLoop。它死循环阻塞在 socket.read() 上。  
   * **逻辑**：只负责从 Socket 读取字节流，通过 Kryo 反序列化为对象，然后**立即**放入内部的 rcvQueue (接收队列)。  
   * **意义**：确保 TCP 缓冲区永远被及时清空，防止因逻辑线程卡顿（比如正在 waitFor）导致网络层面的 TCP Window Full 阻塞。  
2. **VT2 \- 发送线程 (IO-Send / The Mouth)**  
   * **职责**：运行 RpcSession.sendLoop。它阻塞在 sndQueue.take() 上。  
   * **逻辑**：一旦业务层调用 sendNext()，它就取出对象，序列化并写入 Socket。  
   * **意义**：实现了完全的**全双工 (Full Duplex)** 通信。发送数据不再受接收逻辑或业务逻辑的阻塞影响。  
3. **VT3 \- 逻辑线程 (Processor / The Brain)**  
   * **职责**：运行 NetworkSession.readLoop。它阻塞在 rcvQueue.poll() 上。  
   * **逻辑**：这是我们编写业务逻辑的地方（如 ClientPsHandler）。它从队列取出数据包，判断是 RPC 响应还是普通消息，并执行 waitFor 的匹配逻辑或路由到 NetworkRouter。  
   * **意义**：这正是我们可以放心地写 Thread.sleep 或 future.get() 的地方。因为它的阻塞只会挂起这个虚拟线程，绝不会影响底层的 IO 收发。

这种**IO与逻辑分离**的设计，让我们既拥有了 NIO 的高性能吞吐，又拥有了 BIO 的简单编程模型。

## **五、 技术总结**

1.2G2 并没有引入新的游戏玩法，但它重塑了服务端的血管。

* **NetworkSession** 现在继承自 RpcSession，运行在独立的虚拟线程中。  
* **读写分离**：发送队列（Send Loop）和接收循环（Receive Loop）分离，但业务逻辑通过 waitFor 实现了同步化。  
* **线性逻辑**：复杂的握手状态机被简化为线性的函数执行流。

我们坚信，**让代码易于理解，是系统稳定性的最大保障**。通过回归 BIO 模型，我们让网络编程变得前所未有的简单。  
感谢大家的支持！